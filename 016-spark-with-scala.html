<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Exploring Spark with Scala | Crash Course on Apache Spark</title>

    <!-- Styles and fonts -->
    <link href="themes/glide/stylesheets/glide.css" rel="stylesheet" type="text/css" />
    <link href="themes/glide/stylesheets/syntax.css" rel="stylesheet" type="text/css" />
    <link href='//fonts.googleapis.com/css?family=Bree+Serif&subset=latin,latin-ext' rel='stylesheet' type='text/css'>

    <!-- Place favicon.ico and apple-touch-icon.png in the root directory -->

    <script src="themes/glide/javascripts/modernizr.js" type="text/javascript"></script>
  </head>

  <body class="x016-spark-with-scala glide">
    <div class="page-wrapper">

      <!--[if lt IE 7]>
          <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
      <![endif]-->

      <header class="page-header" role="banner">
<a class="invisilink" href="./">            <span class="book-title">Crash Course on Apache Spark</span>
</a>        <span class="book-author visuallyhidden">by <a href="http://linkedin.com/in/saptak">Saptak Sen</a></span>
      </header>

        <div class="content-wrapper clearfix">

          <section class="main-content" role="main">
            <div class="main-content-source">
              <h1>Exploring Spark with Scala</h1>

<p>In this section we are going to walk through the process of using Scala and Apache Spark to interactively analyze data on a Apache Hadoop Cluster.</p>

<p>By the end of this tutorial, you will have learned:</p>

<ol>
<li>How to interact with Apache Spark through an interactive Spark shell</li>
<li>How to read a text file from HDFS and create a RDD</li>
<li>How to interactively analyze a data set through a rich set of Spark API operations</li>
</ol>

<p>Let’s open a shell to our Sandbox through SSH:</p>

<p><img src="https://www.dropbox.com/s/tzsxvsnxfo26jn7/Screenshot_2015-04-13_07_58_43.png?dl=1" /></p>

<p>The default password is <code>hadoop</code> if you are login in into a Sandbox running on your machine.</p>

<p>Now let’s start the Spark Shell</p>
<pre class="highlight shell">spark-shell --master yarn-client --driver-memory 512m --executor-memory 512m
</pre>

<p><img src="https://www.dropbox.com/s/nmuwjfn7i6j0jia/Screenshot%202015-06-08%2008.06.32.png?dl=1" /></p>

<p>There is a :sh command in the Spark shell that lets you run linux commands:</p>
<pre class="highlight scala"><span class="k">:</span><span class="kt">sh</span> <span class="kt">sudo</span> <span class="kt">jps</span>
</pre>

<p><img src="https://www.dropbox.com/s/wypitay56i5xpwy/Screenshot%202015-06-08%2008.08.42.png?dl=1" /></p>

<p>The res0 output that you see in my case stands for ‘result #0’.</p>

<p>Now, print the output of result #0, which is the output of the commandline <code>jps</code> in our case:</p>
<pre class="highlight scala"><span class="n">res0</span><span class="o">.</span><span class="n">show</span>
</pre>

<p><img src="https://www.dropbox.com/s/elzn77ewphy8eir/Screenshot%202015-06-08%2008.10.58.png?dl=1" /></p>

<p>When we launched the Spark shell, more JVMs has been instantiated to support the Shell, namely the SparkSubmit and CoarseGrainedExecutorBackend.</p>

<p>The SparkSubmit is the driver for the &lsquo;Spark shell&rsquo; application and the CoarseGrainedExecutorBackend is the Executor running to support our application.</p>

<p>You can always exit the Spark shell by pressing <code>CTRL+D</code>.</p>

<p>Next, let&rsquo;s get some data into the Sandbox by copy-pasting the following into a new file called <code>littlelog.csv</code>, and then save it on your sandbox in the hdfs <code>/tmp</code> directory:</p>
<pre class="highlight plaintext">20120315 01:17:06,99.122.210.248,[http://www.acme.com/SH55126545/VD55170364,{7AAB8415-E803-3C5D-7100-E362D7F67CA7},homestead,fl,usa](http://www.acme.com/SH55126545/VD55170364,{7AAB8415-E803-3C5D-7100-E362D7F67CA7},homestead,fl,usa)

20120315 01:34:46,69.76.12.213,[http://www.acme.com/SH55126545/VD55177927,{8D0E437E-9249-4DDA-BC4F-C1E5409E3A3B},coeur d alene,id,usa](http://www.acme.com/SH55126545/VD55177927,{8D0E437E-9249-4DDA-BC4F-C1E5409E3A3B},coeur d alene,id,usa)

20120315 17:23:53,67.240.15.94,[http://www.acme.com/SH55126545/VD55166807,{E3FEBA62-CABA-11D4-820E-00A0C9E58E2D},queensbury,ny,usa](http://www.acme.com/SH55126545/VD55166807,{E3FEBA62-CABA-11D4-820E-00A0C9E58E2D},queensbury,ny,usa)

20120315 17:05:00,67.240.15.94,[http://www.acme.com/SH55126545/VD55149415,{E3FEBA62-CABA-11D4-820E-00A0C9E58E2D},queensbury,ny,usa](http://www.acme.com/SH55126545/VD55149415,{E3FEBA62-CABA-11D4-820E-00A0C9E58E2D},queensbury,ny,usa)

20120315 01:27:53,98.234.107.75,[http://www.acme.com/SH55126545/VD55179433,{49E0D2EE-1D57-48C5-A27D-7660C78CB55C},sunnyvale,ca,usa](http://www.acme.com/SH55126545/VD55179433,{49E0D2EE-1D57-48C5-A27D-7660C78CB55C},sunnyvale,ca,usa)

20120315 02:09:38,75.85.165.38,[http://www.acme.com/SH55126545/VD55179433,{F6F8B460-4204-4C26-A32C-B93826EDCB99},san diego,ca,usa](http://www.acme.com/SH55126545/VD55179433,{F6F8B460-4204-4C26-A32C-B93826EDCB99},san diego,ca,usa)
</pre>

<p><img src="https://www.dropbox.com/s/3djm8kuxtt3mri4/Screenshot%202015-06-08%2008.21.53.png?dl=1" /></p>

<p>Put the file <code>littlelog.csv</code> into /tmp directory in hadoop:</p>
<pre class="highlight shell">hadoop fs -put ./littlelog.csv /tmp/
</pre>

<p><img src="https://www.dropbox.com/s/kt2ee75ytn3kmfp/Screenshot%202015-06-08%2008.25.17.png?dl=1" /></p>

<p>Now we have our data in HDFS, let&rsquo;s launch <code>spark-shell</code></p>
<pre class="highlight plaintext">spark-shell --master yarn-client --driver-memory 512m --executor-memory 512m
</pre>

<p><img src="https://www.dropbox.com/s/ry9ygu7c61ilcz7/Screenshot%202015-06-08%2008.33.54.png?dl=1" /></p>

<p>In Spark, datasets are represented as a list of entries, where the list is broken up into many different partitions that are each stored on a different machine. Each partition holds a unique subset of the entries in the list. Spark calls datasets that it stores &ldquo;Resilient Distributed Datasets&rdquo; (RDDs).</p>

<p>So let&rsquo;s create a RDD from our <code>littlelog.csv</code>:</p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">file</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">"hdfs://sandbox.hortonworks.com:8020/tmp/littlelog.csv"</span><span class="o">)</span>
</pre>

<p>Now we have a freshly created RDD. We have to use an action operation like <code>collect()</code> to gather up the data into the drivers memory and then to print out the contents of the file:</p>
<pre class="highlight plaintext">file.collect().foreach(println)
</pre>

<p><img src="https://www.dropbox.com/s/cclf7ttf45i7xtq/Screenshot%202015-06-08%2008.58.24.png?dl=1" /></p>

<p>Remember doing a <code>collect()</code> action operation on a very large distributed RDD can cause your driver program to run out of memory and crash. So, do not use <code>collect()</code> except for when you are prototyping your Spark program on a small dataset.</p>

<p>Another way to print the content of the RDD is</p>
<pre class="highlight plaintext">file.toArray.foreach(println)
</pre>

<p><img src="https://www.dropbox.com/s/odczf5d1ipjw2fw/Screenshot%202015-06-10%2007.33.44.png?dl=1" /></p>

<p>In fact you can easily discover other methods that apply to this RDD by tab auto-completion.</p>

<p>Type the name of the RDD followed by a <code>.</code>, in our case it&rsquo;s <code>file.</code> and the press the <code>&lt;TAB&gt;</code> key.</p>

<p><img src="https://www.dropbox.com/s/lvg7fcmj7a728ni/Screenshot%202015-06-10%2007.37.45.png?dl=1" /></p>

<p>Now let’s extract some information from this data.</p>

<p>Let’s create a map where the state is the key and the number of visitors is the value.</p>

<p>Since state is the 6th element in each row of our text in <code>littlelog.csv</code> (index 5), we need to use a map operator to pass in the lines of text to a function that will parse out the 6th element and store it in a new RDD containing two elements as the key, then count the number of times it appears in the set and provide that number as the value in the second element of this new RDD.</p>

<p>By using the Spark API operator map, we have created or transformed our original RDD into a newer one.</p>

<p>So let’s do it step by step. First let’s filter out the blank lines.</p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">fltr</span> <span class="k">=</span> <span class="n">file</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/edl1v0bb04bde5s/Screenshot%202015-06-08%2009.22.54.png?dl=1" /></p>

<p>WAIT! What is that _ doing there? _ is a shortcut or wildcard in Scala that essentially means ‘whatever happens to be passed to me’. So in the above code the _ stands for each row of our file RDD and we are saying fltr equals a new RDD that is composed of each row with a length &gt; 0.</p>

<p>So, we are invoking the method length on an unknown ‘whatever’ and trusting that Scala will figure out that the thing in each row of the file RDD is actually a String that supports the length operator.</p>

<p>So, in other words within the parenthesis of our filter method we are defining the argument: ‘whatever’, and the logic to be applied to it.</p>

<p>This pattern of constructing a function within the argument to a method is one of the fundamental characteristics of Scala and once you get used to it, it will make sense and speed up your programming a lot.</p>

<p>Then let’s split the line into individual columns seperated by space and then let’s grab the 5th columns</p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">keys</span> <span class="k">=</span> <span class="n">fltr</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">","</span><span class="o">)).</span><span class="n">map</span><span class="o">(</span><span class="n">a</span> <span class="k">=&gt;</span> <span class="n">a</span><span class="o">(</span><span class="mi">5</span><span class="o">))</span>
</pre>

<p><img src="https://www.dropbox.com/s/3kl2r73fj3hex33/Screenshot%202015-06-08%2009.25.24.png?dl=1" /></p>

<p>Notice that we are using the ‘whatever’ shortcut again. This time each row of the fltr RDD is having the split(“,”) method called on it, resulting in an anonymous RDD which we are then invoking map on and defining a function with the strange syntax =&gt; which stands for, ‘what is before me is the variable name (the type is inferred), what is after me is what you do to it’. In this case, each row (an array) in the anonymous RDD created by split is, in turn, assigned to the variable ‘a’ and then we extract the 5th element from it, which ends up being added to the named RDD called ‘keys’ we declared at the start of the line of code.</p>

<p>Then let’s print out the values of the key.</p>
<pre class="highlight scala"><span class="n">keys</span><span class="o">.</span><span class="n">collect</span><span class="o">().</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/38czyv8k6z5knqt/Screenshot%202015-06-08%2009.27.35.png?dl=1" /></p>

<p>Notice that some of the states are not unique and repeat. We need to count how many times each key (state) appears in the log.</p>

<p>Now let’s generate a key-value pair for each state as the key and the corresponding value as 1.</p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">stateCnt</span> <span class="k">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">key</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">key</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span>
</pre>

<p><img src="https://www.dropbox.com/s/2ydp5z7ndm8h3gh/Screenshot%202015-06-08%2009.29.05.png?dl=1" /></p>

<p>Next, we will iterate through each row of the stateCnt RDD and pass their contents to a utility method available to our RDD that counts the distinct number of rows containing each key</p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">lastMap</span> <span class="k">=</span> <span class="n">stateCnt</span><span class="o">.</span><span class="n">countByKey</span>
</pre>

<p><img src="https://www.dropbox.com/s/wg8fojy5x5zem84/Screenshot%202015-06-08%2009.33.56.png?dl=1" /></p>

<p>Now, let’s print out the result.</p>
<pre class="highlight scala"><span class="n">lastMap</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</pre>

<p>Result: a listing of state abbreviations and the count of how many times visitors from that state hit our website.</p>
<pre class="highlight plaintext">(ny,2)
(ca,2)
(fl,1)
(id,1)
</pre>

<p><img src="https://www.dropbox.com/s/fu4n9h6u257d3ge/Screenshot%202015-06-08%2009.34.58.png?dl=1" /></p>

<p>Note that at this point you still have access to all the RDDs you have created during this session. You can reprocess any one of them, for instance, again printing out the values contained in the keys RDD:</p>
<pre class="highlight scala"><span class="n">keys</span><span class="o">.</span><span class="n">collect</span><span class="o">().</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/6t85jvmyt7ud1xr/Screenshot%202015-06-08%2009.35.58.png?dl=1" /></p>

<p>I hope this has proved informative and that you have enjoyed this simple example of how you can interact with Data on HDP using Scala and Apache Spark.</p>

            </div>
            <nav class="main-content-nav clearfix" role="navigation">
              <ul>
                <li><a class="previous" href="013-scala-primer.html">Previous</a></li>
                <li><a class="next" href="019-hive-orc-spark.html">Next</a></li>
              </ul>
            </nav>
          </section>

          <aside class="aside-content" role="complementary">
            <div class="aside-wrapper">

              <div class="block table-of-contents">
                <h3>Table of Contents</h3>
                <nav role="navigation">
                  <ul>
                    <li class='child '><a href="./">Introduction</a></li><li class='child '><a href="001-configuring-hortonwork-sandbox-azure.html">Configuring Hortonworks Sandbox on Azure</a></li><li class='child '><a href="004-installing-apache-spark-1-3-1.html">Installing Apache Spark 1.3.1 on HDP 2.2.4.2</a></li><li class='child '><a href="007-Installing-Spark-1-2.html">Installing Apache Spark 1.2.0 on HDP 2.2</a></li><li class='child '><a href="010-basics-of-programming-apache-spark.html">Basics of programming Apache Spark</a></li><li class='child '><a href="013-scala-primer.html">A short primer on Scala</a></li><li class='child active'><a href="016-spark-with-scala.html">Exploring Spark with Scala</a></li><li class='child '><a href="019-hive-orc-spark.html">Using Hive and ORC with Apache Spark</a></li><li class='child '><a href="022-installing-zeppelin.html">Installing and configuring Zeppelin</a></li><li class='child '><a href="025-ipython-notebook-with-apache-spark.html">Using IPython Notebook with Apache Spark</a></li><li class='child '><a href="028-spark-with-hdp.html">A Lap around Apache Spark 1.3.1 with HDP 2.3</a></li>
                  </ul>
                </nav>
              </div>
              <!--
              <div class="block translations">
                <h3>Book translations</h3>
                <p>This book is translated into 
                  <a href="#">Spanish</a>
                  and <a href="#">English</a>.
                  You can help with more translations <a href="https://github.com/saptak/spark">on Github</a>.
                </p>
              </div>
              -->
            </div><!-- end aside-wrapper -->
          </aside>

        </div><!-- end content-wrapper -->

      <footer class="page-footer">

        <div class="footer-wrapper">
          <!--
          <div class="block downloads">&nbsp;
            <h3>Downloads</h3>
            <p>Download this book in
                <a href="#">PDF</a>, <a href="#">mobi</a>, and <a href="#">epub</a>
              form for free.
            </p>
          </div>
          -->
          <div class="block license">
            <h3>License</h3>
            <p>This book is licensed under the <a href="https://creativecommons.org/licenses/by-sa/4.0">Attribution-ShareAlike</a> license.</p>
          </div>

          <div class="block open-source">
            <h3>This book is open source</h3>
            <p>The source of this book is <a href="https://github.com/saptak/spark">hosted on Github</a>. Go, ahead. Check it out.</p>
          </div>

          <small class="small-text">Made with <a href="http://bitbooks.cc">Bitbooks</a></small>

        </div><!-- end footer-wrapper -->

      </footer>


    </div><!-- end page-wrapper -->

    <!-- include scripts just before the close of the body tag -->
    <script src="themes/glide/javascripts/anchor.min.js" type="text/javascript"></script>
    <script src="themes/glide/javascripts/glide.js" type="text/javascript"></script>
  </body>
</html>