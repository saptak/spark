<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>A Lap around Apache Spark 1.3.1 with HDP 2.3 | Crash Course on Apache Spark</title>

    <!-- Styles and fonts -->
    <link href="themes/glide/stylesheets/glide.css" rel="stylesheet" type="text/css" />
    <link href="themes/glide/stylesheets/syntax.css" rel="stylesheet" type="text/css" />
    <link href='//fonts.googleapis.com/css?family=Bree+Serif&subset=latin,latin-ext' rel='stylesheet' type='text/css'>

    <!-- Place favicon.ico and apple-touch-icon.png in the root directory -->

    <script src="themes/glide/javascripts/modernizr.js" type="text/javascript"></script>
  </head>

  <body class="x028-spark-with-hdp glide">
    <div class="page-wrapper">

      <!--[if lt IE 7]>
          <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
      <![endif]-->

      <header class="page-header" role="banner">
<a class="invisilink" href="./">            <span class="book-title">Crash Course on Apache Spark</span>
</a>        <span class="book-author visuallyhidden">by <a href="http://linkedin.com/in/saptak">Saptak Sen</a></span>
      </header>

        <div class="content-wrapper clearfix">

          <section class="main-content" role="main">
            <div class="main-content-source">
              <h1>A Lap around Apache Spark 1.3.1 with HDP 2.3</h1>

<p>This Apache Spark 1.3.1 with HDP 2.3 guide walks you through many of the newer features of Apache Spark 1.3.1 on YARN.</p>

<p>Hortonworks recently announced general availability of Spark 1.3.1 on the HDP platform. Apache Spark is a fast moving community and Hortonworks plans frequent releases to allow evaluation and production use of the latest capabilities of Apache Spark on HDP for our customers.</p>

<p>With YARN, Hadoop can now support many types of data and application workloads; Spark on YARN becomes yet another workload running against the same set of hardware resources.</p>

<p>This guide describes how to:</p>

<ul>
<li>Run Spark on YARN and run the canonical Spark examples: SparkPi and Wordcount.</li>
<li>Run Spark 1.3.1 on HDP 2.3.</li>
<li>Use Spark DataFrame API</li>
<li>Work with a built-in UDF, collect_list, a key feature of Hive 13. This release provides support for Hive 0.13.1 and instructions on how to call this UDF from Spark shell.</li>
<li>Use SparkSQL thrift JDBC/ODBC Server.</li>
<li>View history of finished jobs with Spark Job History.</li>
<li>Use ORC files with Spark, with examples.</li>
</ul>

<p>When you are ready to go beyond these tasks, try the machine learning examples at Apache Spark.</p>

<h3>HDP Cluster Requirement</h3>

<p>Spark 1.3.1 can be configured on any HDP 2.3 cluster whether it is a multi node cluster or a single node HDP Sandbox.</p>

<p>The instructions in this guide assumes you are using the latest Hortonworks Sandbox</p>

<h3>Run the Spark Pi Example</h3>

<p>To test compute intensive tasks in Spark, the Pi example calculates pi by “throwing darts” at a circle. The example points in the unit square ((0,0) to (1,1)) and sees how many fall in the unit circle. The fraction should be pi/4, which is used to estimate Pi.</p>

<p>To calculate Pi with Spark:</p>

<ol>
<li><strong>Change to your Spark directory and become spark OS user:</strong></li>
</ol>
<pre class="highlight shell"><span class="nb">cd</span> /usr/hdp/current/spark-client  
su spark
</pre>

<ol>
<li><strong>Run the Spark Pi example in yarn-client mode:</strong></li>
</ol>
<pre class="highlight shell">./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 3 --driver-memory 512m --executor-memory 512m --executor-cores 1 lib/spark-examples<span class="k">*</span>.jar 10
</pre>

<p><strong>Note:</strong> The Pi job should complete without any failure messages and produce output similar to below, note the value of Pi in the output message:
<img src="https://www.dropbox.com/s/i93qmcvjmzue1ho/Screenshot%202015-07-20%2014.48.48.png?dl=1" /></p>

<h3>Using WordCount with Spark</h3>

<h4>Copy input file for Spark WordCount Example</h4>

<p>Upload the input file you want to use in WordCount to HDFS. You can use any text file as input. In the following example, log4j.properties is used as an example:</p>

<p>As user spark:</p>
<pre class="highlight shell">hadoop fs -copyFromLocal /etc/hadoop/conf/log4j.properties /tmp/data
</pre>

<h3>Run Spark WordCount</h3>

<p>To run WordCount:</p>

<h4>Run the Spark shell:</h4>
<pre class="highlight shell">./bin/spark-shell --master yarn-client --driver-memory 512m --executor-memory 512m
</pre>

<p>Output similar to below displays before the Scala REPL prompt, scala&gt;:
<img src="https://www.dropbox.com/s/f5bq5biq88gc2bs/Screenshot%202015-07-20%2014.50.31.png?dl=1" /></p>

<h4>At the Scala REPL prompt enter:</h4>
<pre class="highlight scala"><span class="k">val</span> <span class="n">file</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">"/tmp/data"</span><span class="o">)</span>
<span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">file</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)).</span><span class="n">map</span><span class="o">(</span><span class="n">word</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="mi">1</span><span class="o">)).</span><span class="n">reduceByKey</span><span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">)</span>
<span class="n">counts</span><span class="o">.</span><span class="n">saveAsTextFile</span><span class="o">(</span><span class="s">"/tmp/wordcount"</span><span class="o">)</span>
</pre>

<h5>Viewing the WordCount output with Scala Shell</h5>

<p>To view the output in the scala shell:</p>
<pre class="highlight scala"><span class="n">counts</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
</pre>

<p><img src="https://www.dropbox.com/s/0j1yk2okljlqx3k/Screenshot%202015-07-20%2014.55.13.png?dl=1" /></p>

<p>To print the full output of the WordCount job:</p>
<pre class="highlight scala"><span class="n">counts</span><span class="o">.</span><span class="n">toArray</span><span class="o">().</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/942krdi729qbkzx/Screenshot%202015-07-20%2014.57.10.png?dl=1" /></p>

<h5>Viewing the WordCount output with HDFS</h5>

<p>To read the output of WordCount using HDFS command:
Exit the scala shell.</p>
<pre class="highlight scala"><span class="n">exit</span>
</pre>

<p>View WordCount Results:</p>
<pre class="highlight shell">hadoop fs -ls /tmp/wordcount
</pre>

<p>It should display output similar to:</p>

<p><img src="https://www.dropbox.com/s/tg17aqu3kieb9wa/Screenshot%202015-07-20%2014.58.22.png?dl=1" /></p>

<p>Use the HDFS cat command to see the WordCount output. For example,</p>
<pre class="highlight shell">hadoop fs -cat /tmp/wordcount/part-00000
</pre>

<p><img src="https://www.dropbox.com/s/xed8ikl35jj8dx7/Screenshot%202015-07-20%2014.59.10.png?dl=1" /></p>

<h5>Using Spark DataFrame API</h5>

<p>With Spark 1.3.1, DataFrame API is a new feature. DataFrame API provide easier access to data since it looks conceptually like a Table and a lot of developers from Python/R/Pandas are familiar with it.</p>

<p>Let&rsquo;s upload people text file to HDFS</p>
<pre class="highlight shell"><span class="nb">cd</span> /usr/hdp/current/spark-client

su spark
hdfs dfs -copyFromLocal examples/src/main/resources/people.txt people.txt

hdfs dfs -copyFromLocal examples/src/main/resources/people.json people.json
</pre>

<p><img src="https://www.dropbox.com/s/g8igatmz9v7n4hy/Screenshot%202015-07-20%2015.01.49.png?dl=1" /></p>

<p>Then let&rsquo;s launch the Spark Shell</p>
<pre class="highlight shell"><span class="nb">cd</span> /usr/hdp/current/spark-client
su spark
./bin/spark-shell --num-executors 2 --executor-memory 512m --master yarn-client
</pre>

<p>At the Spark Shell type the following:</p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">jsonFile</span><span class="o">(</span><span class="s">"people.json"</span><span class="o">)</span>
</pre>

<p>This will produce and output such as</p>

<p><img src="https://www.dropbox.com/s/7dk2nsnfrvkv4y4/Screenshot%202015-07-20%2015.04.33.png?dl=1" /></p>

<p><strong>Note:</strong> The highlighted output shows the inferred schema of the underlying people.json.</p>

<p>Now print the content of DataFrame with df.show</p>
<pre class="highlight scala"><span class="n">df</span><span class="o">.</span><span class="n">show</span>
</pre>

<p><img src="https://www.dropbox.com/s/keq8tjfkz4fjfjb/Screenshot%202015-07-20%2015.06.29.png?dl=1" /></p>

<h5>Data Frame API examples</h5>
<pre class="highlight scala"><span class="k">import</span> <span class="nn">org.apache.spark.sql.functions._</span>
<span class="c1">// Select everybody, but increment the age by 1
</span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">df</span><span class="o">(</span><span class="s">"name"</span><span class="o">),</span> <span class="n">df</span><span class="o">(</span><span class="s">"age"</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
</pre>

<p><img src="https://www.dropbox.com/s/n8lmf0lg4ed6t54/Screenshot%202015-07-20%2015.13.59.png?dl=1" /></p>
<pre class="highlight scala"><span class="c1">// Select people older than 21
</span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">df</span><span class="o">(</span><span class="s">"age"</span><span class="o">)</span> <span class="o">&gt;</span> <span class="mi">21</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
</pre>

<p><img src="https://www.dropbox.com/s/2yjemit47m1eo3v/Screenshot%202015-07-20%2015.14.47.png?dl=1" /></p>
<pre class="highlight scala"><span class="c1">// Count people by age
</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">"age"</span><span class="o">).</span><span class="n">count</span><span class="o">().</span><span class="n">show</span><span class="o">()</span>
</pre>

<p><img src="https://www.dropbox.com/s/y5mk0zdxzi8si0t/Screenshot%202015-07-20%2015.15.41.png?dl=1" /></p>

<h5>Programmatically Specifying Schema</h5>
<pre class="highlight scala"><span class="k">import</span> <span class="nn">org.apache.spark.sql._</span>
<span class="k">val</span> <span class="n">sqlContext</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="nc">SQLContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span>
<span class="k">val</span> <span class="n">people</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">"people.txt"</span><span class="o">)</span>
<span class="k">val</span> <span class="n">schemaString</span> <span class="k">=</span> <span class="s">"name age"</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.types.</span><span class="o">{</span><span class="nc">StructType</span><span class="o">,</span><span class="nc">StructField</span><span class="o">,</span><span class="nc">StringType</span><span class="o">}</span>
<span class="k">val</span> <span class="n">schema</span> <span class="k">=</span> <span class="nc">StructType</span><span class="o">(</span><span class="n">schemaString</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">fieldName</span> <span class="k">=&gt;</span> <span class="nc">StructField</span><span class="o">(</span><span class="n">fieldName</span><span class="o">,</span> <span class="nc">StringType</span><span class="o">,</span> <span class="kc">true</span><span class="o">)))</span>
</pre>

<p><img src="https://www.dropbox.com/s/21m81un74ml65dd/Screenshot%202015-07-20%2015.18.02.png?dl=1" /></p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">rowRDD</span> <span class="k">=</span> <span class="n">people</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">","</span><span class="o">)).</span><span class="n">map</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="nc">Row</span><span class="o">(</span><span class="n">p</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">p</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">trim</span><span class="o">))</span>
<span class="k">val</span> <span class="n">peopleDataFrame</span> <span class="k">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="o">(</span><span class="n">rowRDD</span><span class="o">,</span> <span class="n">schema</span><span class="o">)</span>
<span class="n">peopleDataFrame</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"people"</span><span class="o">)</span>
<span class="k">val</span> <span class="n">results</span> <span class="k">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"SELECT name FROM people"</span><span class="o">)</span>
<span class="n">results</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">t</span> <span class="k">=&gt;</span> <span class="s">"Name: "</span> <span class="o">+</span> <span class="n">t</span><span class="o">(</span><span class="mi">0</span><span class="o">)).</span><span class="n">collect</span><span class="o">().</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</pre>

<p>This will produce an output like</p>

<p><img src="https://www.dropbox.com/s/t3x20c5fs2dh2o1/Screenshot%202015-07-20%2015.19.49.png?dl=1" /></p>

<h3>Running Hive 0.13.1 UDF</h3>

<p>Hive 0.13.1 provides a new built-in UDF collect_list(col) which returns a list of objects with duplicates.
The below example reads and write to HDFS under Hive directories. In a production environment one needs appropriate HDFS permission. However for evaluation you can run all this section as hdfs user.</p>

<p>Before running Hive examples run the following steps:</p>

<h4>Launch Spark Shell on YARN cluster</h4>
<pre class="highlight scala"><span class="n">su</span> <span class="n">hdfs</span>
<span class="o">./</span><span class="n">bin</span><span class="o">/</span><span class="n">spark</span><span class="o">-</span><span class="n">shell</span> <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">executors</span> <span class="mi">2</span> <span class="o">--</span><span class="n">executor</span><span class="o">-</span><span class="n">memory</span> <span class="mi">512</span><span class="n">m</span> <span class="o">--</span><span class="n">master</span> <span class="n">yarn</span><span class="o">-</span><span class="n">client</span>
</pre>

<h4>Create Hive Context</h4>
<pre class="highlight scala"><span class="k">val</span> <span class="n">hiveContext</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">hive</span><span class="o">.</span><span class="nc">HiveContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span>
</pre>

<p>You should see output similar to the following:</p>

<p><img src="https://www.dropbox.com/s/aga459qghah9x15/Screenshot%202015-07-20%2015.24.12.png?dl=1" /></p>

<h4>Create Hive Table</h4>
<pre class="highlight scala"><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"CREATE TABLE IF NOT EXISTS TestTable (key INT, value STRING)"</span><span class="o">)</span>
</pre>

<p>You should see output similar to the following:</p>

<p><img src="https://www.dropbox.com/s/h23w2qf99arqfjk/Screenshot%202015-07-20%2015.25.34.png?dl=1" /></p>

<h4>Load example KV value data into Table</h4>
<pre class="highlight plaintext">scala&gt; hiveContext.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE TestTable")
</pre>

<p>You should see output similar to the following:</p>

<p><img src="https://www.dropbox.com/s/pqu1u95ep91omna/Screenshot%202015-07-20%2015.26.53.png?dl=1" /></p>

<h4>Invoke Hive collect_list UDF</h4>
<pre class="highlight scala"><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"from TestTable SELECT key, collect_list(value) group by key order by key"</span><span class="o">).</span><span class="n">collect</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</pre>

<p>You should see output similar to the following:</p>

<p><img src="https://www.dropbox.com/s/3j0qfu95gxdvg2u/Screenshot%202015-07-21%2010.40.04.png?dl=1" /></p>

<h3>Read &amp; Write ORC File Example</h3>

<p>In this tech preview, we have implemented full support for ORC files with Spark. We will walk through an example that reads and write ORC file and uses ORC structure to infer a table.</p>

<h3>ORC File Support</h3>

<h4>Create a new Hive Table with ORC format</h4>
<pre class="highlight scala"><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"create table orc_table(key INT, value STRING) stored as orc"</span><span class="o">)</span>
</pre>

<h4>Load Data into the ORC table</h4>
<pre class="highlight scala"><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"INSERT INTO table orc_table select * from testtable"</span><span class="o">)</span>
</pre>

<h4>Verify that Data is loaded into the ORC table</h4>
<pre class="highlight scala"><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"FROM orc_table SELECT *"</span><span class="o">).</span><span class="n">collect</span><span class="o">().</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/ufqqxnpq25uktt6/Screenshot%202015-07-21%2010.42.11.png?dl=1" /></p>

<h4>Read ORC Table from HDFS as HadoopRDD**</h4>
<pre class="highlight scala"><span class="k">val</span> <span class="n">inputRead</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">hadoopFile</span><span class="o">(</span><span class="s">"/apps/hive/warehouse/orc_table"</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat</span><span class="o">],</span><span class="n">classOf</span><span class="o">[</span><span class="kt">org.apache.hadoop.io.NullWritable</span><span class="o">],</span><span class="n">classOf</span><span class="o">[</span><span class="kt">org.apache.hadoop.hive.ql.io.orc.OrcStruct</span><span class="o">])</span>
</pre>

<p><img src="https://www.dropbox.com/s/ivc37b3vykknsfb/Screenshot%202015-07-21%2010.45.03.png?dl=1" /></p>

<h4>Verify we can manipulate the ORC record through RDD</h4>
<pre class="highlight scala"><span class="k">val</span> <span class="n">k</span> <span class="k">=</span> <span class="n">inputRead</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">pair</span> <span class="k">=&gt;</span> <span class="n">pair</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">toString</span><span class="o">)</span>
<span class="k">val</span> <span class="n">c</span> <span class="k">=</span> <span class="n">k</span><span class="o">.</span><span class="n">collect</span>
</pre>

<p>You should see output similar to the following:</p>

<p><img src="https://www.dropbox.com/s/yqk5a39iep23lby/Screenshot%202015-07-21%2010.46.20.png?dl=1" /></p>

<h4><strong>Copy example table into HDFS</strong></h4>
<pre class="highlight shell"><span class="nb">cd</span> /usr/hdp/current/spark-client
su spark
hadoop dfs -put examples/src/main/resources/people.txt people.txt
</pre>

<h4>Run Spark-Shell</h4>
<pre class="highlight shell">./bin/spark-shell --num-executors 2 --executor-memory 512m --master yarn-client
</pre>

<p>on Scala prompt type the following, except for the comments</p>
<pre class="highlight scala"><span class="k">import</span> <span class="nn">org.apache.spark.sql.hive.orc._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.types.</span><span class="o">{</span><span class="nc">StructType</span><span class="o">,</span><span class="nc">StructField</span><span class="o">,</span><span class="nc">StringType</span><span class="o">}</span>
<span class="k">val</span> <span class="n">hiveContext</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">hive</span><span class="o">.</span><span class="nc">HiveContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/6qinxmfpwdmvzon/Screenshot%202015-07-21%2010.54.29.png?dl=1" /></p>

<p>Load and register the spark table</p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">people</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">"people.txt"</span><span class="o">)</span>
<span class="k">val</span> <span class="n">schemaString</span> <span class="k">=</span> <span class="s">"name age"</span>
<span class="k">val</span> <span class="n">schema</span> <span class="k">=</span> <span class="nc">StructType</span><span class="o">(</span><span class="n">schemaString</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">fieldName</span> <span class="k">=&gt;</span> <span class="nc">StructField</span><span class="o">(</span><span class="n">fieldName</span><span class="o">,</span> <span class="nc">StringType</span><span class="o">,</span> <span class="kc">true</span><span class="o">)))</span>
<span class="k">val</span> <span class="n">rowRDD</span> <span class="k">=</span> <span class="n">people</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">","</span><span class="o">)).</span><span class="n">map</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="nc">Row</span><span class="o">(</span><span class="n">p</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="k">new</span> <span class="nc">Integer</span><span class="o">(</span><span class="n">p</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">trim</span><span class="o">)))</span>
</pre>

<p><img src="https://www.dropbox.com/s/xqmvdgs14b08yak/Screenshot%202015-07-21%2013.35.47.png?dl=1" /></p>

<p>Infer table schema from RDD</p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">peopleSchemaRDD</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">applySchema</span><span class="o">(</span><span class="n">rowRDD</span><span class="o">,</span> <span class="n">schema</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/n5heod5udrwsxm3/Screenshot%202015-07-21%2013.37.06.png?dl=1" /></p>

<p>Create a table from schema</p>
<pre class="highlight scala"><span class="n">peopleSchemaRDD</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"people"</span><span class="o">)</span>
<span class="k">val</span> <span class="n">results</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"SELECT * FROM people"</span><span class="o">)</span>
<span class="n">results</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">t</span> <span class="k">=&gt;</span> <span class="s">"Name: "</span> <span class="o">+</span> <span class="n">t</span><span class="o">.</span><span class="n">toString</span><span class="o">).</span><span class="n">collect</span><span class="o">().</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/fngxh0pmxmcwdvb/Screenshot%202015-07-21%2013.43.49.png?dl=1" /></p>

<p>Save Table to ORCFile</p>
<pre class="highlight scala"><span class="n">peopleSchemaRDD</span><span class="o">.</span><span class="n">saveAsOrcFile</span><span class="o">(</span><span class="s">"people.orc"</span><span class="o">)</span>
</pre>

<p>Create Table from ORCFile</p>
<pre class="highlight scala"><span class="k">val</span> <span class="n">morePeople</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">orcFile</span><span class="o">(</span><span class="s">"people.orc"</span><span class="o">)</span>
<span class="n">morePeople</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"morePeople"</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/ub7o8isy6b0gfqk/Screenshot%202015-07-21%2013.46.51.png?dl=1" /></p>

<p>Query from the table</p>
<pre class="highlight scala"><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"SELECT * from morePeople"</span><span class="o">).</span><span class="n">collect</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</pre>

<p><img src="https://www.dropbox.com/s/fo6ryfluwqn323i/Screenshot%202015-07-21%2013.47.58.png?dl=1" /></p>

<h3>SparkSQL Thrift Server for JDBC/ODBC access</h3>

<p>With this release SparkSQL’s thrift server provides JDBC access to SparkSQL.</p>

<ol>
<li><strong>Start Thrift Server</strong>
From SPARK_HOME, start SparkSQL thrift server, Note the port value of the thrift JDBC server</li>
</ol>
<pre class="highlight scala"><span class="n">su</span> <span class="n">spark</span>
<span class="o">./</span><span class="n">sbin</span><span class="o">/</span><span class="n">start</span><span class="o">-</span><span class="n">thriftserver</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">master</span> <span class="n">yarn</span><span class="o">-</span><span class="n">client</span> <span class="o">--</span><span class="n">executor</span><span class="o">-</span><span class="n">memory</span> <span class="mi">512</span><span class="n">m</span> <span class="o">--</span><span class="n">hiveconf</span> <span class="n">hive</span><span class="o">.</span><span class="n">server2</span><span class="o">.</span><span class="n">thrift</span><span class="o">.</span><span class="n">port</span><span class="k">=</span><span class="mi">10001</span>
</pre>

<ul>
<li><strong>Connect to Thrift Server over beeline</strong>
Launch beeline from SPARK_HOME</li>
</ul>
<pre class="highlight scala"><span class="n">su</span> <span class="n">spark</span>
<span class="o">./</span><span class="n">bin</span><span class="o">/</span><span class="n">beeline</span>
</pre>

<ul>
<li><strong>Connect to Thrift Server &amp; Issue SQL commands</strong>
On beeline prompt</li>
</ul>
<pre class="highlight sql"><span class="o">!</span><span class="k">connect</span> <span class="n">jdbc</span><span class="p">:</span><span class="n">hive2</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">10001</span>
</pre>

<p>Note this is example is without security enabled, so any username password should work.</p>

<p>Note, the connection may take a few second to be available and try show tables after a wait of 10-15 second in a Sandbox env.</p>
<pre class="highlight sql"><span class="k">show</span> <span class="n">tables</span><span class="p">;</span>
</pre>

<p><img src="https://www.dropbox.com/s/cwgmi3dbhm6566y/Screenshot%202015-07-21%2013.53.04.png?dl=1" /></p>

<p>type <code>Ctrl+C</code> to exit beeline.</p>

<ul>
<li><strong>Stop Thrift Server</strong></li>
</ul>
<pre class="highlight shell">./sbin/stop-thriftserver.sh
</pre>

<p><img src="https://www.dropbox.com/s/b6mvjepkfuadkxi/Screenshot%202015-07-21%2013.55.40.png?dl=1" /></p>

<h3>Spark Job History Server</h3>

<p>Spark Job history server is integrated with YARN’s Application Timeline Server(ATS) and publishes job metrics to ATS. This allows job details to be available after the job finishes.</p>

<ol>
<li><strong>Start Spark History Server</strong></li>
</ol>
<pre class="highlight shell">./sbin/start-history-server.sh
</pre>

<p>You can let the history server run, while you run examples and go to YARN resource manager page at <a href="http://127.0.0.1:8088/cluster/apps">http://127.0.0.1:8088/cluster/apps</a> and see the logs of finished application with the history server.</p>

<ol>
<li><strong>Stop Spark History Server</strong></li>
</ol>
<pre class="highlight shell">./sbin/stop-history-server.sh
</pre>

<p><img src="https://www.dropbox.com/s/5p14qxkuqw6r9hg/Screenshot%202015-07-21%2014.00.10.png?dl=1" /></p>

<p>Visit <a href="http://hortonworks.com/tutorials">http://hortonworks.com/tutorials</a> for more tutorials on Apache Spark.</p>

            </div>
            <nav class="main-content-nav clearfix" role="navigation">
              <ul>
                <li><a class="previous" href="025-ipython-notebook-with-apache-spark.html">Previous</a></li>
                <li></li>
              </ul>
            </nav>
          </section>

          <aside class="aside-content" role="complementary">
            <div class="aside-wrapper">

              <div class="block table-of-contents">
                <h3>Table of Contents</h3>
                <nav role="navigation">
                  <ul>
                    <li class='child '><a href="./">Introduction</a></li><li class='child '><a href="001-configuring-hortonwork-sandbox-azure.html">Configuring Hortonworks Sandbox on Azure</a></li><li class='child '><a href="004-installing-apache-spark-1-3-1.html">Installing Apache Spark 1.3.1 on HDP 2.2.4.2</a></li><li class='child '><a href="007-Installing-Spark-1-2.html">Installing Apache Spark 1.2.0 on HDP 2.2</a></li><li class='child '><a href="010-basics-of-programming-apache-spark.html">Basics of programming Apache Spark</a></li><li class='child '><a href="013-scala-primer.html">A short primer on Scala</a></li><li class='child '><a href="016-spark-with-scala.html">Exploring Spark with Scala</a></li><li class='child '><a href="019-hive-orc-spark.html">Using Hive and ORC with Apache Spark</a></li><li class='child '><a href="022-installing-zeppelin.html">Installing and configuring Zeppelin</a></li><li class='child '><a href="025-ipython-notebook-with-apache-spark.html">Using IPython Notebook with Apache Spark</a></li><li class='child active'><a href="028-spark-with-hdp.html">A Lap around Apache Spark 1.3.1 with HDP 2.3</a></li>
                  </ul>
                </nav>
              </div>
              <!--
              <div class="block translations">
                <h3>Book translations</h3>
                <p>This book is translated into 
                  <a href="#">Spanish</a>
                  and <a href="#">English</a>.
                  You can help with more translations <a href="https://github.com/saptak/spark">on Github</a>.
                </p>
              </div>
              -->
            </div><!-- end aside-wrapper -->
          </aside>

        </div><!-- end content-wrapper -->

      <footer class="page-footer">

        <div class="footer-wrapper">
          <!--
          <div class="block downloads">&nbsp;
            <h3>Downloads</h3>
            <p>Download this book in
                <a href="#">PDF</a>, <a href="#">mobi</a>, and <a href="#">epub</a>
              form for free.
            </p>
          </div>
          -->
          <div class="block license">
            <h3>License</h3>
            <p>This book is licensed under the <a href="https://creativecommons.org/licenses/by-sa/4.0">Attribution-ShareAlike</a> license.</p>
          </div>

          <div class="block open-source">
            <h3>This book is open source</h3>
            <p>The source of this book is <a href="https://github.com/saptak/spark">hosted on Github</a>. Go, ahead. Check it out.</p>
          </div>

          <small class="small-text">Made with <a href="http://bitbooks.cc">Bitbooks</a></small>

        </div><!-- end footer-wrapper -->

      </footer>


    </div><!-- end page-wrapper -->

    <!-- include scripts just before the close of the body tag -->
    <script src="themes/glide/javascripts/anchor.min.js" type="text/javascript"></script>
    <script src="themes/glide/javascripts/glide.js" type="text/javascript"></script>
  </body>
</html>